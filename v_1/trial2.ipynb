{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytesseract \n",
    "# !pip install torch torchvision transformers timm opencv-python numpy sounddevice \n",
    "# !pip install torch torchvision transformers timm opencv-python numpy sounddevice\n",
    "# !pip install pytesseract \n",
    "# !pip install transformers\n",
    "# !pip install pyttsx3 pyaudio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Reek\\Documents\\Luminate\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Using cache found in C:\\Users\\Reek/.cache\\torch\\hub\\intel-isl_MiDaS_master\n",
      "c:\\Users\\Reek\\Documents\\Luminate\\.venv\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Reek/.cache\\torch\\hub\\rwightman_gen-efficientnet-pytorch_master\n",
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 768,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Text: iF\n",
      "\n",
      "Detected Text: q\n",
      "\n",
      "Detected Text: La |\n",
      "L*\n",
      "e%\n",
      "aver\n",
      "4\n",
      "\n",
      "Detected Text: |\n",
      "\n",
      "Detected Text: iF\n",
      "\n",
      "Detected Text: Kid\n",
      "\n",
      "Detected Text: 4 ;\n",
      "E\n",
      "‘oy\n",
      "\n",
      "Detected Text: YF\n",
      "\n",
      "Detected Text: ‘\\\n",
      "\n",
      "Detected Text: l,\n",
      "\n",
      "Detected Text: i\n",
      "\n",
      "Detected Text: =|\n",
      "\n",
      "Detected Text: .\n",
      "4\n",
      "\n",
      "Detected Text: =,\n",
      "\n",
      "Detected Text: —\n",
      "\n",
      "Detected Text: \\,\n",
      "\n",
      "Detected Text: ‘a\n",
      "\n",
      "Detected Text: Y\n",
      "\n",
      "Exiting program...\n",
      "Camera feed closed. All resources released.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection, AutoProcessor, AutoModelForSeq2SeqLM\n",
    "import torchvision.transforms as T\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pyttsx3  # For text-to-speech\n",
    "from transformers import VisionEncoderDecoderModel\n",
    "\n",
    "# ============================\n",
    "# Load Models\n",
    "# ============================\n",
    "\n",
    "# 1. Load DETR Model for Object Detection (Hugging Face)\n",
    "detr_processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "detr_model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "# 2. Load MiDaS for Depth Estimation (Torch Hub)\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS_small\")\n",
    "midas.eval()\n",
    "\n",
    "# 3. Load OCR Model (TrOCR for text recognition)\n",
    "ocr_processor = AutoProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\",use_fast=True)\n",
    "# ocr_model = AutoModelForSeq2SeqLM.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "ocr_model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "\n",
    "# 4. Configure Tesseract OCR Path (Ensure Tesseract is installed)\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "# 5. Initialize Text-to-Speech Engine\n",
    "tts = pyttsx3.init()\n",
    "\n",
    "# ============================\n",
    "# Camera & Processing Setup\n",
    "# ============================\n",
    "\n",
    "# Open camera feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Define average human step in feet (adjustable)\n",
    "avg_step_feet = 2.5  # Estimated step size for a human\n",
    "\n",
    "# Preprocessing transformations for MiDaS (Depth Estimation)\n",
    "midas_transform = T.Compose([\n",
    "    T.Resize((384, 384)),  # Resize image for MiDaS\n",
    "    T.ToTensor(),  # Convert image to tensor\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize for MiDaS\n",
    "])\n",
    "\n",
    "# ============================\n",
    "# Real-Time Processing Loop\n",
    "# ============================\n",
    "try:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # Stop if camera feed fails\n",
    "\n",
    "        # Convert OpenCV image to PIL format for DETR processing\n",
    "        pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        inputs = detr_processor(images=pil_image, return_tensors=\"pt\")\n",
    "\n",
    "        # Object Detection using DETR\n",
    "        with torch.no_grad():\n",
    "            outputs = detr_model(**inputs)\n",
    "\n",
    "        # Post-process DETR output\n",
    "        target_sizes = torch.tensor([pil_image.size[::-1]])\n",
    "        results = detr_processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.7)[0]\n",
    "\n",
    "        # Depth Estimation using MiDaS\n",
    "        img_tensor = midas_transform(pil_image).unsqueeze(0)  # Preprocess image for MiDaS\n",
    "        with torch.no_grad():\n",
    "            depth_map = midas(img_tensor)\n",
    "\n",
    "        # Resize depth map to match input image dimensions\n",
    "        depth_map = depth_map.squeeze().cpu().numpy()\n",
    "        depth_map = cv2.resize(depth_map, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "        # ============================\n",
    "        # Process Detected Objects\n",
    "        # ============================\n",
    "\n",
    "        for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "            x1, y1, x2, y2 = map(int, box.tolist())  # Extract bounding box coordinates\n",
    "            class_name = detr_model.config.id2label[label.item()]  # Get object label\n",
    "\n",
    "            # Estimate Distance (Average depth value inside bounding box)\n",
    "            avg_depth = np.mean(depth_map[y1:y2, x1:x2])\n",
    "            distance_ft = avg_depth * 3.28  # Convert meters to feet\n",
    "            steps_to_object = distance_ft / avg_step_feet  # Estimate number of steps\n",
    "\n",
    "            # Draw Bounding Box & Display Distance on Frame\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            text = f\"{class_name} | {distance_ft:.1f} ft | Steps: {steps_to_object:.1f}\"\n",
    "            cv2.putText(frame, text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "            # ============================\n",
    "            # Text Recognition (OCR) for Objects\n",
    "            # ============================\n",
    "\n",
    "            obj_crop = frame[y1:y2, x1:x2]  # Crop detected object\n",
    "            gray_crop = cv2.cvtColor(obj_crop, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "            detected_text = pytesseract.image_to_string(gray_crop, config=\"--psm 6\")  # OCR Processing\n",
    "\n",
    "            if detected_text.strip():  # If text is detected\n",
    "                print(f\"Detected Text: {detected_text}\")\n",
    "                tts.say(detected_text)  # Read detected text aloud\n",
    "                tts.runAndWait()  # Wait for speech to complete\n",
    "                cv2.putText(frame, f\"Text: {detected_text}\", (x1, y2 + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)\n",
    "\n",
    "        # Show the processed frame\n",
    "        cv2.imshow(\"Object Detection, Distance Estimation, & OCR\", frame)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord(\"q\") or key == 27:  # Press 'q' or ESC to quit\n",
    "            print(\"Exiting program...\")\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Program interrupted by user. Exiting...\")\n",
    "\n",
    "finally:\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Camera feed closed. All resources released.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
